This `TROUBLESHOOTING.md` document outlines the key technical challenges encountered during the platform build, their root causes, and the SRE-focused resolutions. This serves as a runbook for future maintenance and demonstrates advanced debugging skills.

## TROUBLESHOOTING.md

### 1. Hybrid Architecture & Connectivity

This section covers issues related to the multi-cluster setup, networking, and the hybrid control/data plane configuration.

| Issue | Root Cause | Resolution |
| :--- | :--- | :--- |
| **K3s & ARM64 Architecture Incompatibility** | Attempting to use an `arm64` k3s binary (compiled on a Mac) on the GCP `e2-small` instance, which uses an `amd64` architecture. | **Action:** Always verify the target architecture. Downloaded the correct `amd64` k3s binary directly onto the GCP VM using `curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.33 sh -` to ensure architecture compatibility. |
| **Firewall Problems due to Missing Server/Cluster Labels** | The GCP firewall rule was configured to target specific network tags (e.g., `k3s-server`) but the Compute Engine VM was missing the required label, blocking `kubectl` traffic. | **Action:** Verified the firewall rule target tags. Applied the correct network tag to the GCP VM instance via the GCP Console or `gcloud compute instances add-tags`. **Verification:** Confirmed the `kubectl` traffic (port 6443) was now allowed. |
| **K3s & WSL2 Incompatibility** | Native k3s installation within WSL2 often conflicts with the Windows/WSL2 NAT networking layer, leading to unreliable cluster communication and internal IP routing failures. | **Action:** Switched from native k3s on WSL2 to **k3d (k3s-in-Docker)**. k3d leverages Docker's more robust and predictable networking, bypassing the common WSL2 networking issues and providing a more reliable simulation of an edge/containerized deployment. |
| **Incorrect `$KUBECONFIG` Mapping** | The `KUBECONFIG` environment variable in the Windows/WSL2 environment was not correctly set or exported to point to the newly generated k3d kubeconfig file. | **Action:** Explicitly set and verified the environment variable in the shell (PowerShell or WSL2 Bash) before running `kubectl` commands. **Verification:** Used `echo $env:KUBECONFIG` (PowerShell) or `echo $KUBECONFIG` (WSL2) and confirmed `kubectl config view` showed the correct cluster context. |
| **Wrong K3d Cluster Configuration** | The k3d cluster was initially created with `--servers 0`, preventing the initialization of the control plane and a valid kubeconfig file. | **Action:** Deleted the broken cluster and recreated it with a minimum of one server node (`--servers 1`) to establish the control plane, as detailed in the master context. |
| **Missing Exposed Ports** | The initial k3d cluster creation did not map all necessary NodePorts (e.g., for the ML inference endpoint `30808`) from the container to the Windows host, making them inaccessible to the GCP control plane via Tailscale. | **Action:** Performed a cluster recreation with a comprehensive port mapping strategy, ensuring all required NodePorts were explicitly exposed via the `--port` flag during `k3d cluster create`. |
| **K3d Cluster’s TLS Certificate didn’t match Tailscale’s IP** | ArgoCD on the GCP cluster could not securely connect to the k3d API server because the k3s TLS certificate only contained the internal Docker IP, not the external Tailscale IP. | **Action:** Recreated the k3d cluster, explicitly including the Tailscale IP in the Subject Alternative Names (SANs) for the k3s server certificate using the flag: `--k3s-arg "--tls-san=<TAILSCALE_IP>@server:*"`. |

### 2. Platform Component Deployment & Stability

This section addresses issues related to the core platform services (ArgoCD, Monitoring) and resource constraints.

| Issue | Root Cause | Resolution |
| :--- | :--- | :--- |
| **Corrupted K3s cluster after heavy port-forwarding** | The GCP `e2-small` instance (2GB RAM) ran out of memory during prolonged `kubectl port-forward` sessions, leading to an abrupt system crash and partial corruption of the k3s control plane state. | **Action:** **1. Rescue VM:** Detached the disk and mounted it to a temporary VM to repair SSH access. **2. Controlled Recovery:** Reattached the disk and performed a controlled restart of failed k3s components by deleting affected pods (`kubectl delete pod -n <namespace> -l <label>`). ArgoCD and k3s self-healed, restoring the cluster state from Git. **Prevention:** Implemented aggressive resource limits on all platform pods. |
| **GitOps sync issues (Missing ArgoCD Application)** | The CI/CD pipeline updated the `deployment.yaml` with a new image tag, but the ArgoCD `Application` resource, which tells ArgoCD *what* to sync, was missing from the repository. | **Action:** Created and committed the `Application` manifest (`iris-classifier-app.yaml`) to the `infra-manifests` repository, ensuring ArgoCD was explicitly configured to watch the target path and sync the application to the `ml-worker` cluster. |
| **K3d `ml-apps` namespace not automatically created** | The model deployment failed because the target namespace (`ml-apps`) did not exist on the k3d cluster. | **Action:** Ensured the ArgoCD `Application` manifest included the `syncOptions` to automatically create the namespace if it does not exist: `syncOptions: - CreateNamespace=true`. |
| **ArgoCD CRDs not installed in K3d Cluster** | An ArgoCD-related error appeared on the k3d worker cluster, indicating a missing Custom Resource Definition (CRD). | **Action:** Verified that only the necessary application manifests (Deployment, Service) were being deployed to the k3d cluster. ArgoCD's core components and CRDs must only reside on the **GCP Control Plane** cluster. Corrected the ArgoCD `Application` manifest to target only the application path. |
| **Prometheus Port Not Exposed in K3d** | Prometheus on the GCP cluster could not scrape metrics from the k3d cluster because the k3d NodePort for Prometheus (30900) was not mapped to the host. | **Action:** Recreated the k3d cluster, ensuring the Prometheus NodePort was explicitly mapped: `--port "30900:30900@agent:0"`. **Verification:** Confirmed the Prometheus target was `UP` in the Prometheus UI. |

### 3. CI/CT/CD Pipeline Failures

This section details errors encountered during the Continuous Integration, Training, and Deployment stages.

| Issue | Root Cause | Resolution |
| :--- | :--- | :--- |
| **Dependency Incompatibility** | The `requirements.txt` file used for training and inference contained conflicting or unpinned versions of core libraries (e.g., `scikit-learn`, `numpy`), leading to inconsistent builds. | **Action:** Pinned all dependencies to specific, verified versions (e.g., `scikit-learn==1.5.2`). Ensured the same `requirements.txt` was used in both the training stage and the Docker build to guarantee environment parity. |
| **Dependencies not properly copied to the final stage** | The multi-stage Docker build installed Python dependencies in the `builder` stage, but the final, minimal stage did not copy the necessary site-packages or set the correct `PATH`. | **Action:** Updated the `Dockerfile` to explicitly copy the installed packages from the builder stage and set the `PATH` environment variable for the non-root user: `COPY --from=builder /root/.local /root/.local` and `ENV PATH=/root/.local/bin:$PATH`. |
| **Docker image tag causing build errors** | The GitHub Container Registry (GHCR) requires repository names to be lowercase, but the GitHub repository name contained uppercase letters. | **Action:** Used a shell command in the GitHub Actions workflow to force the repository name to lowercase before tagging the image: `IMAGE_NAME_LOWER=$(echo "${{ github.repository }}" \| tr '[:upper:]' '[:lower:]')`. Used this lowercase variable for all tagging and pushing operations. |
| **Stage 6 (GitOps update) failing with empty image tag** | The image tag output from Stage 5 was incorrectly referenced in Stage 6, resulting in an empty variable being passed to the GitOps update script. | **Action:** Corrected the `needs` output reference in Stage 6 to point to the correct step ID and output name: `image-tag: ${{ needs.build-and-scan.outputs.image-tag }}`. |
| **Deployment manifest updated incorrectly** | The `sed` command used in the GitOps update stage was executed with an empty image tag variable, resulting in an empty `image:` field in the `deployment.yaml` manifest. | **Action:** Implemented a robust check for an empty variable (`if [ -z "$NEW_TAG" ]; then exit 1; fi`) and refined the `sed` command to use a more resilient regex pattern that preserves indentation and correctly replaces the image value: `sed -i "s|^\(\s*image:\).*|\1 ${NEW_TAG}|g" applications/iris-classifier/deployment.yaml`. |